{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "from logging import getLogger\n",
    "from config import YandexEnvELZ\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_id = YandexEnvELZ.folder\n",
    "model = \"yandexgpt\"\n",
    "api_key = YandexEnvELZ.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prompt_tokens(text, folder_id, api_key):\n",
    "    prompt = {\"modelUri\": f\"gpt://{folder_id}/{model}/latest\", \"text\": text}\n",
    "    url_tokenize = \"https://llm.api.cloud.yandex.net/foundationModels/v1/tokenize\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Api-Key {api_key}\",\n",
    "        \"x-data-logging-enabled\": \"false\",\n",
    "    }\n",
    "    #\n",
    "    response = requests.post(url_tokenize, headers=headers, json=prompt)\n",
    "    text_new = re.sub(r\"true\", r\"True\", response.text)\n",
    "    text_new = re.sub(r\"false\", r\"False\", text_new)\n",
    "\n",
    "    return len(eval(text_new)[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yandex_generate(prompt_list):\n",
    "    \n",
    "    temperature= 0\n",
    "    max_tokens = 500    \n",
    "\n",
    "        \n",
    "    messages_prompt = [\n",
    "            {\"role\": \"system\", \"text\": prompt_list[0]},\n",
    "            {\"role\": \"user\", \"text\": prompt_list[1]},  \n",
    "            #1 пример\n",
    "            {\"role\": \"user\", \"text\": prompt_list[2]},  \n",
    "            {\"role\": \"assistant\", \"text\": prompt_list[3]},\n",
    "            #2 пример\n",
    "            {\"role\": \"user\", \"text\": prompt_list[4]},  \n",
    "            {\"role\": \"assistant\", \"text\": prompt_list[5]},\n",
    "            #3 пример\n",
    "            {\"role\": \"user\", \"text\": prompt_list[6]},  \n",
    "            {\"role\": \"assistant\", \"text\": prompt_list[7]},\n",
    "            #4 пример\n",
    "            {\"role\": \"user\", \"text\": prompt_list[8]},  \n",
    "            {\"role\": \"assistant\", \"text\": prompt_list[9]},\n",
    "            #5 пример\n",
    "            {\"role\": \"user\", \"text\": prompt_list[10]},  \n",
    "            {\"role\": \"assistant\", \"text\": prompt_list[11]},\n",
    "\n",
    "            {\"role\": \"user\", \"text\": prompt_list[12]} \n",
    "            \n",
    "        ]\n",
    "\n",
    "    prompt = {\n",
    "        \"modelUri\": f\"gpt://{folder_id}/{model}/latest\",\n",
    "        \"completionOptions\": {\n",
    "            \"stream\": False,\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "        },\n",
    "        \"messages\": messages_prompt,\n",
    "    }\n",
    "    url = \"https://llm.api.cloud.yandex.net/foundationModels/v1/completion\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Api-Key {api_key}\",\n",
    "        \"x-data-logging-enabled\": \"false\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=prompt)  \n",
    "    print(response.text)\n",
    "    result = eval(response.text)[\"result\"][\"alternatives\"][0][\"message\"][\"text\"]\n",
    "    \n",
    "    \n",
    "    return result\n",
    "yandex_generate(prompt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle, json, string, torch\n",
    "from datetime import datetime\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, set_seed\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import BitsAndBytesConfig,HfArgumentParser,TrainingArguments, logging, DataCollatorForLanguageModeling, Trainer\n",
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"MalakhovIlya/NEREL\")\n",
    "folders = [\"train\", \"test\", \"dev\"]\n",
    "\n",
    "\n",
    "#@title функции для изъятия сущностей\n",
    "#!!! - то, что было в изначальном коде\n",
    "def get_list_values(text):\n",
    "    return text.split()\n",
    "\n",
    "def replc_t_n(text):\n",
    "    return re.sub(\"\\t|\\n\", \" \", text).strip()\n",
    "\n",
    "def grouped_and_sort_labeled_data(annotation_file):\n",
    "\n",
    "    # df_ann = pd.DataFrame([get_list_values(replc_t_n(i)) for i in annotation_file if \";\" not in i])\n",
    "    df_ann = pd.DataFrame([get_list_values(i) for i in annotation_file  if \";\" not in i])\n",
    "    df_ann[2] = df_ann[2].astype(\"int\")\n",
    "    df_ann[3] = df_ann[3].astype(\"int\")\n",
    "\n",
    "\n",
    "    '''\n",
    "    df_ann.to_csv(\"df_ann.csv\")\n",
    "    # (1869-окончание) - если группировать, то исчезает прокуратура Бишкека как сущность\n",
    "    # есть отдельно прокуратура и отдельно Бишкек\n",
    "    #!!! grouped = df_ann.groupby([1, 2])[3].min().reset_index()\n",
    "    grouped.to_csv(\"grouped.csv\")\n",
    "    print(grouped)\n",
    "    '''\n",
    "    return df_ann.sort_values(by=2)[[1,2,3]].values #!!! grouped.sort_values(by=2)[[1,2,3]].values\n",
    "\n",
    "def split_text_on_labeled_tokens(text, labels):\n",
    "    def chunk_text_labeling(text, start, end, is_ner = False):\n",
    "        # print(f\"start = {start}, end = {end}\\n  text[start: end]={text[start: end]}\")\n",
    "        chunk_iter = 0\n",
    "        ner_chunk = text[start: end].split()\n",
    "        for part_of_chunk in ner_chunk:\n",
    "            split_text.append(part_of_chunk)\n",
    "            if is_ner:\n",
    "                if chunk_iter == 0:\n",
    "                    ner_label.append(\"B-\"+ner)\n",
    "                else:\n",
    "                    ner_label.append(\"I-\"+ner)\n",
    "                chunk_iter += 1\n",
    "            else:\n",
    "                ner_label.append(\"O\")\n",
    "    ### inner function\n",
    "\n",
    "    init_start = 0\n",
    "    split_text = []\n",
    "    ner_label = []\n",
    "    for ner, start, end in labels:\n",
    "        if start > init_start:\n",
    "\n",
    "            chunk_text_labeling(text, init_start, start)\n",
    "            chunk_text_labeling(text, start, end, True)\n",
    "        else:\n",
    "            chunk_text_labeling(text, start, end, True)\n",
    "        init_start = end\n",
    "\n",
    "    return split_text, ner_label\n",
    "\n",
    "\n",
    "#@title функции для очистки токенов\n",
    "\n",
    "''' в разделенных словах есть знаки препинания - нужно убрать\n",
    "split_text_sent = ['\"В', 'ходе', 'проведения', 'оперативно-профилактического', 'мероприятия', 'под', 'кодовым', 'названием', '\"', 'Арсенал', '\"', 'в',\n",
    "'''\n",
    "\n",
    "def starts_or_ends_with_punctuation(s):\n",
    "    punct=string.punctuation\n",
    "    punct+=' '\n",
    "    return [(len(s)!=1 and s[0] in punct), (len(s)!=1 and s[-1] in punct)]\n",
    "def clear_tokens(token, label):\n",
    "\n",
    "  find_punct_start, find_punct_end = starts_or_ends_with_punctuation(token)\n",
    "  if  find_punct_start==0 and find_punct_end==0: # token==punctuation mark\n",
    "    return [token], [label]\n",
    "  elif find_punct_start==1 and find_punct_end==0:\n",
    "    return [token[0], token[1:]], [\"O\", label]\n",
    "  elif find_punct_start==0 and find_punct_end==1:\n",
    "    return [token[:-1], token[-1]], [label, \"O\"]\n",
    "  else:\n",
    "    if (len(token)==2): #случаи '#!'\n",
    "       return [token[0], token[-1]], [\"O\", \"O\"]\n",
    "    else:\n",
    "      return [token[0], token[1:-1], token[-1]], [\"O\", label, \"O\"]\n",
    "#пример\n",
    "text = ['боеприпасов.', 'пресс-служба', '\"В', '.', '\"В.', \"!#\"]\n",
    "labels=['SOMETHING', 'SOMETHING', 'O', 'O', 'O', 'O']\n",
    "for i_text in text:\n",
    "  print(f\"{i_text}   ==>  {starts_or_ends_with_punctuation(i_text)}\")\n",
    "new_texts, new_labels = [], []\n",
    "for i in range(len(text)):\n",
    "  new_texts+=(clear_tokens(text[i], labels[i])[0])\n",
    "  new_labels+=(clear_tokens(text[i], labels[i])[1])\n",
    "print(new_texts)\n",
    "print(new_labels)\n",
    "\n",
    "for folder in folders:\n",
    "  all_sequences = []\n",
    "  all_labels = []\n",
    "  df_folder = pd.DataFrame()\n",
    "  for i_seq in dataset[f\"{folder}\"]:\n",
    "    ann = i_seq['entities']\n",
    "    txt = i_seq['text']\n",
    "\n",
    "    '''проверка на пустые файлы'''\n",
    "    if len(ann) == 0:\n",
    "        continue\n",
    "\n",
    "    '''разделение текста и токенов'''\n",
    "    labels = grouped_and_sort_labeled_data(ann) #[['ORGANIZATION' 2501 2519], ....]\n",
    "    split_text, ner_label = split_text_on_labeled_tokens(txt, labels)\n",
    "\n",
    "    '''разделение текста на предложения'''\n",
    "    start_iter = 0\n",
    "    ner_label_sent = []\n",
    "    split_text_sent =[]\n",
    "    for i in range(len(ner_label)):\n",
    "      if split_text[i].endswith('.') or split_text[i].endswith('!') or split_text[i].endswith('?'):\n",
    "        ner_label_sent.append(ner_label[start_iter:i+1])\n",
    "        split_text_sent.append(split_text[start_iter:i+1])\n",
    "        start_iter = i+1\n",
    "\n",
    "    '''очистка токенов\n",
    "    почему: в разделенных словах есть знаки препинания - нужно убрать\n",
    "    пример: ['\"В', 'ходе', 'проведения',\n",
    "    '''\n",
    "    text_data, label_data =[], []\n",
    "    for sent_label, sentence in zip(ner_label_sent, split_text_sent):\n",
    "      new_texts, new_labels = [], []\n",
    "      for i in range(len(sent_label)):\n",
    "        new_texts+=(clear_tokens(sentence[i], sent_label[i])[0])\n",
    "        new_labels+=(clear_tokens(sentence[i], sent_label[i])[1])\n",
    "      text_data.append(new_texts)\n",
    "      label_data.append(new_labels)\n",
    "\n",
    "    # print(f\"final text = {text_data}\")\n",
    "    # print(f\"final label= {label_data}\")\n",
    "\n",
    "    df_folder = pd.concat([df_folder,pd.DataFrame({\"tokens\": text_data, \"labels\": label_data})])\n",
    "\n",
    "  df_folder = df_folder.reset_index().drop('index', axis=1)\n",
    "\n",
    "  # '''файлы для дообучения моделей deeppavlov'''\n",
    "  # file_tr = open(f'/content/deeppavlov_tuning/{folder}.txt', \"w\")\n",
    "  # for row in range(len(df_folder)):\n",
    "  #   for i in range(len(df_folder['sequences'][row])):\n",
    "  #     file_tr.writelines(f\"{df_folder['sequences'][row][i]} {df_folder['labels'][row][i]}\\n\")\n",
    "  #   file_tr.writelines(f\"\\n\")\n",
    "  # file_tr.close()\n",
    "\n",
    "  with open(f'C:/Users/User/Desktop/PhD/changed_NEREL/{folder}_data.pickle', 'wb') as f:\n",
    "    pickle.dump(df_folder, f)\n",
    "    df_folder.to_csv(f\"C:/Users/User/Desktop/PhD/changed_NEREL/df_{folder}.csv\")\n",
    "  print(f\"For folder <{folder}> prepared <{df_folder.shape[0]}> sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('C:/Users/User/Desktop/PhD/changed_NEREL/train_data.pickle', 'rb')\n",
    "train_data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file1 = open('C:/Users/User/Desktop/PhD/changed_NEREL/test_data.pickle', 'rb')\n",
    "test_data = pickle.load(file1)\n",
    "file1.close()\n",
    "\n",
    "file2 = open('C:/Users/User/Desktop/PhD/changed_NEREL/dev_data.pickle', 'rb')\n",
    "dev_data = pickle.load(file2)\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity=\"FAMILY\"\n",
    "\n",
    "# зануление ненужных сущностей\n",
    "def replace_tags(tags, entity):\n",
    "    return ['O' if tag not in ['O', f'B-{entity}', f'I-{entity}'] else tag for tag in tags]\n",
    "\n",
    "\n",
    "def get_needed_entity(entity):\n",
    "    few_shot_text = dev_data[dev_data['labels'].apply(lambda x: f'B-{entity}' in x)]\n",
    "    few_shot_text['labels'] = few_shot_text.apply(lambda x: replace_tags(x['labels'], entity),axis=1)\n",
    "    \n",
    "    few_shot_text=few_shot_text.reset_index().drop(\"index\", axis=1)\n",
    "   \n",
    "\n",
    "    few_shot_text1 = test_data[test_data['labels'].apply(lambda x: f'B-{entity}' in x)]\n",
    "    few_shot_text1['labels'] = few_shot_text1.apply(lambda x: replace_tags(x['labels'], entity),axis=1)\n",
    "    few_shot_text1=few_shot_text1.reset_index().drop(\"index\", axis=1)\n",
    "    \n",
    "\n",
    "    few_shot_check = train_data[train_data['labels'].apply(lambda x: f'B-{entity}' in x)]\n",
    "    few_shot_check['labels'] = few_shot_check.apply(lambda x: replace_tags(x['labels'], entity),axis=1)\n",
    "    few_shot_check=few_shot_check.reset_index().drop(\"index\", axis=1)\n",
    "    return  few_shot_text, few_shot_text1, few_shot_check\n",
    "few_shot_text, few_shot_text1, few_shot_check = get_needed_entity(entity)\n",
    "\n",
    "\n",
    "def extract_entity(df, entity):\n",
    "    entities_list=[]\n",
    "    for i in range(len(df)):\n",
    "        entities = []\n",
    "        bool_check=False\n",
    "        for i_word in range(len(df.iloc[i]['tokens'])):\n",
    "            if df.iloc[i]['labels'][i_word]==f'B-{entity}':\n",
    "                bool_check=True\n",
    "                find_entity = df.iloc[i]['tokens'][i_word]\n",
    "            elif df.iloc[i]['labels'][i_word]==f'I-{entity}':\n",
    "                find_entity += \" \"  + df.iloc[i]['tokens'][i_word]\n",
    "            elif df.iloc[i]['labels'][i_word]=='O' and bool_check==True:\n",
    "                bool_check=False\n",
    "                entities.append(find_entity)       \n",
    "        entities_list.append(entities)         \n",
    "    df[\"entities\"] = entities_list\n",
    "    df['joined_entities'] = df['entities'].apply(lambda x: ', '.join(x))\n",
    "    return df\n",
    "\n",
    "few_shot_check = extract_entity(few_shot_check, entity)\n",
    "few_shot_text = extract_entity(few_shot_text, entity)\n",
    "few_shot_text1 = extract_entity(few_shot_text1, entity)\n",
    "\n",
    "\n",
    "def remove_repeated_subsequences(text):\n",
    "    words = text.split()\n",
    "    n = len(words)\n",
    "    i = 0\n",
    "    result = []\n",
    "    \n",
    "    while i < n:\n",
    "        found_repetition = False\n",
    "        \n",
    "        for length in range(1, (n - i) // 2 + 1):\n",
    "            subsequence = words[i:i + length]\n",
    "            next_subsequence = words[i + length:i + 2 * length]\n",
    "            if subsequence == next_subsequence:\n",
    "                found_repetition = True\n",
    "                i += length\n",
    "                break\n",
    "        \n",
    "        if not found_repetition:\n",
    "            result.append(words[i])\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "\n",
    "res2 = pd.concat([few_shot_text1, few_shot_text], ignore_index=True) \n",
    "res1 = pd.concat([res2, few_shot_check], ignore_index=True) \n",
    "print(f\"кол-во предложений с сущностью {entity} = {len(few_shot_check)+len(few_shot_text1)+len(few_shot_text)}\")\n",
    "df_example = pd.DataFrame()\n",
    "labels = [5, 14, 20, 26, 37]\n",
    "for i in labels:\n",
    "    df_example = df_example._append(res1.iloc[i], ignore_index=True)\n",
    "df_example\n",
    "df_check = res1.drop(labels=labels, axis=0).reset_index().drop(\"index\", axis=1)\n",
    "df_check.to_csv(\"df_fewshot.csv\")\n",
    "df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_subsequences(text):\n",
    "    words = text.split()\n",
    "    n = len(words)\n",
    "    i = 0\n",
    "    result = []\n",
    "    \n",
    "    while i < n:\n",
    "        found_repetition = False        \n",
    "        for length in range(1, (n - i) // 2 + 1):\n",
    "            subsequence = words[i:i + length]\n",
    "            next_subsequence = words[i + length:i + 2 * length]\n",
    "#             print(f\"subsequence = {subsequence}\\nnext_subsequence={next_subsequence}\\n\")\n",
    "            if subsequence == next_subsequence:\n",
    "                found_repetition = True\n",
    "                i += length\n",
    "                break\n",
    "        \n",
    "        if not found_repetition:\n",
    "            result.append(words[i])\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "# Example usage\n",
    "# input_text = \"9 апреля семья капитана Мура капитана Мура запустила от его имени кампанию по сбору средств для службы здравоохранения на платформе JustGiving\"\n",
    "input_text = 'были и зеленое , зеленое яблоко яблоко яблоко и апельсин и семья капитана Мура капитана Мура'\n",
    "output_text = remove_repeated_subsequences(input_text)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt1 = \"\"\"Ты находишь сущности определенной тематики в предложениях. Сущности - это слова или словосочетания, которые характеризуют определенную тематику.\\\n",
    "Например, сущность (словосочетание)  \\\"11 ноября\\\" характеризует тематику ДАТА. \"\"\"\n",
    "system_prompt2 = \"\"\"Необходимо найти сущности с тематикой СЕМЬЯ. СЕМЬЯ - это сообщество, основанное на браке супругов, помимо которых включает и их детей. СЕМЬЯ состоит из слов: семья, брат, братья, сестра, сестры, клан, мама, папа, муж, жена, род, родственники, родные, близкие, родня; также ассоциируется с фамилиями (в множественном числе) - например, Ивановы, Петровы, Александр и Андрей Федоровы\"\"\"\n",
    "user_prompt1 = remove_repeated_subsequences(' '.join(df_example['tokens'].iloc[0]))\n",
    "assistant_prompt1 = df_example['joined_entities'].iloc[0]\n",
    "\n",
    "user_prompt2 = remove_repeated_subsequences(' '.join(df_example['tokens'].iloc[1]))\n",
    "assistant_prompt2 = df_example['joined_entities'].iloc[1]\n",
    "\n",
    "user_prompt3 = remove_repeated_subsequences(' '.join(df_example['tokens'].iloc[2]))\n",
    "assistant_prompt3 = df_example['joined_entities'].iloc[2]\n",
    "\n",
    "user_prompt4 = remove_repeated_subsequences(' '.join(df_example['tokens'].iloc[3]))\n",
    "assistant_prompt4 = df_example['joined_entities'].iloc[3]\n",
    "\n",
    "user_prompt5 = remove_repeated_subsequences(' '.join(df_example['tokens'].iloc[4]))\n",
    "assistant_prompt5 = df_example['joined_entities'].iloc[4]\n",
    "\n",
    "user_prompt6 = remove_repeated_subsequences(' '.join(df_check['tokens'].iloc[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = orig_stdout\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_stdout = sys.stdout\n",
    "# f = open(os.path.abspath(f\"C:/Users/User/Desktop/PhD/phd/output_yandexgpt.txt\"), 'w', encoding=\"utf-8\")\n",
    "# sys.stdout = f\n",
    "for i in range(len(df_check)):\n",
    "    user_prompt6 = remove_repeated_subsequences(' '.join(df_check['tokens'].iloc[i]))\n",
    "    prompt_list=[system_prompt1, system_prompt2, user_prompt1, assistant_prompt1,\n",
    "                                                 user_prompt2, assistant_prompt2,\n",
    "                                                 user_prompt3, assistant_prompt3, \n",
    "                                                 user_prompt4, assistant_prompt4,\n",
    "                                                 user_prompt5, assistant_prompt5,\n",
    "                                                 user_prompt6]\n",
    "   \n",
    "    print(f\"#{i+1}\")\n",
    "    print(f\"sentance = {user_prompt6}\")\n",
    "    print(yandex_generate(prompt_list))\n",
    "# sys.stdout = orig_stdout\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
